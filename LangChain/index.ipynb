{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶\n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨\n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å†™åœ¨å‰é¢\n",
    "\n",
    "å®˜ç½‘ï¼šhttps://www.langchain.com/\n",
    "\n",
    "- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰\n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹\n",
    "- å­¦ä¹  LangChain è¦å…³æ³¨æ¥å£å˜æ›´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "   - LLMsï¼šå¤§è¯­è¨€æ¨¡å‹\n",
    "   - Chat Modelsï¼šä¸€èˆ¬åŸºäº LLMsï¼Œä½†æŒ‰å¯¹è¯ç»“æ„é‡æ–°å°è£…\n",
    "   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿\n",
    "   - OutputParserï¼šè§£æè¾“å‡º\n",
    "2. æ•°æ®è¿æ¥å°è£…\n",
    "   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨\n",
    "   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œï¼ˆå•¥æ„æ€ï¼Ÿåˆ«æ€¥ï¼Œåé¢è¯¦ç»†è®²ï¼‰\n",
    "   - Verctorstores: ï¼ˆé¢å‘æ£€ç´¢çš„ï¼‰å‘é‡çš„å­˜å‚¨\n",
    "   - Retrievers: å‘é‡çš„æ£€ç´¢\n",
    "3. å¯¹è¯å†å²ç®¡ç†\n",
    "   - å¯¹è¯å†å²çš„å­˜å‚¨ã€åŠ è½½ä¸å‰ªè£\n",
    "4. æ¶æ„å°è£…\n",
    "   - Chainï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ\n",
    "   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½\n",
    "     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰\n",
    "     - Toolkitsï¼šæ“ä½œæŸè½¯ä»¶çš„ä¸€ç»„å·¥å…·é›†ï¼Œä¾‹å¦‚ï¼šæ“ä½œ DBã€æ“ä½œ Gmail ç­‰ç­‰\n",
    "5. Callbacks\n",
    "\n",
    "<img src=\"./assets/langchain.png\" style=\"margin-left: 0px\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–‡æ¡£ï¼ˆä»¥ Python ç‰ˆä¸ºä¾‹ï¼‰\n",
    "\n",
    "- åŠŸèƒ½æ¨¡å—ï¼šhttps://python.langchain.com/docs/get_started/introduction\n",
    "- API æ–‡æ¡£ï¼šhttps://api.python.langchain.com/en/latest/langchain_api_reference.html\n",
    "- ä¸‰æ–¹ç»„ä»¶é›†æˆï¼šhttps://python.langchain.com/docs/integrations/platforms/\n",
    "- å®˜æ–¹åº”ç”¨æ¡ˆä¾‹ï¼šhttps://python.langchain.com/docs/use_cases\n",
    "- è°ƒè¯•éƒ¨ç½²ç­‰æŒ‡å¯¼ï¼šhttps://python.langchain.com/docs/guides/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b> åˆ›å»ºä¸€ä¸ªæ–°çš„ conda ç¯å¢ƒï¼Œ<b>langchain-learn</b>ï¼Œå†å¼€å§‹ä¸‹é¢çš„å­¦ä¹ ï¼\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```powershell\n",
    "conda create -n langchain-learn python=3.10\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚\n",
    "\n",
    "### 1.1 æ¨¡å‹ APIï¼šLLM vs. ChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain\n",
    "# !pip install --upgrade langchain-openai\n",
    "# !pip install --upgrade langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 OpenAI æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å›ç­”é—®é¢˜å’Œæä¾›ä¿¡æ¯ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©çš„åœ°æ–¹ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ä¿è¯æ“ä½œç³»ç»Ÿçš„ç¯å¢ƒå˜é‡é‡Œé¢é…ç½®å¥½äº†OPENAI_API_KEY, OPENAI_BASE_URL\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")  # é»˜è®¤æ˜¯gpt-3.5-turbo\n",
    "response = llm.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯å¤§æ‹¿ï¼Œä¸€ä½å­¦å‘˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role\n",
    "    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role\n",
    "    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯èšå®¢AIç ”ç©¶é™¢çš„è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯å­¦å‘˜ï¼Œæˆ‘å«å¤§æ‹¿ã€‚\"),\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°\")\n",
    "]\n",
    "\n",
    "ret = llm.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º\n",
    "\n",
    "<img src=\"./assets/model_io.jpg\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "#### 1.2.1 Prompt æ¨¡æ¿å°è£…\n",
    "\n",
    "1. PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] input_types={} partial_variables={} template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "===Prompt===\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='å°æ˜'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°æ˜æœ‰ä¸€å¤©å»å‚åŠ ä¸€ä¸ªå­¦æ ¡çš„ç§‘å­¦å±•è§ˆã€‚ä»–çœ‹åˆ°æœ‰ä¸ªåŒå­¦åœ¨å±•ç¤ºä¸€å°å¯ä»¥è‡ªåŠ¨å†™å­—çš„æœºå™¨äººã€‚å°æ˜è§‰å¾—å¾ˆç¥å¥‡ï¼Œå°±é—®åŒå­¦ï¼šâ€œè¿™ä¸ªæœºå™¨äººæ€ä¹ˆèƒ½å†™å­—çš„ï¼Ÿâ€\n",
      "\n",
      "åŒå­¦å¾—æ„åœ°å›ç­”ï¼šâ€œå› ä¸ºå®ƒæœ‰ä¸€ä¸ªè¶…çº§æ™ºèƒ½çš„ç¨‹åºï¼â€\n",
      "\n",
      "å°æ˜æƒ³äº†æƒ³ï¼Œæ‘‡äº†æ‘‡å¤´è¯´ï¼šâ€œé‚£æˆ‘ä¹Ÿè¦ç»™æˆ‘çš„æœºå™¨äººè£…ä¸€ä¸ªè¶…çº§æ™ºèƒ½çš„ç¨‹åºï¼â€\n",
      "\n",
      "åŒå­¦å¥½å¥‡åœ°é—®ï¼šâ€œä½ æ‰“ç®—æ€ä¹ˆåšï¼Ÿâ€\n",
      "\n",
      "å°æ˜è®¤çœŸåœ°è¯´ï¼šâ€œæˆ‘æ‰“ç®—ç»™å®ƒè£…ä¸Šâ€˜æ‡’â€™è¿™ä¸ªç¨‹åºï¼Œè¿™æ ·å®ƒå°±å¯ä»¥å¸®æˆ‘å†™ä½œä¸šäº†ï¼â€\n",
      "\n",
      "åŒå­¦å¿ä¸ä½ç¬‘äº†ï¼šâ€œä½ è¿™æ˜¯åœ¨æ‰¾å€Ÿå£å˜›ï¼â€\n",
      "\n",
      "å°æ˜å¾—æ„åœ°è€¸è€¸è‚©ï¼šâ€œåæ­£æˆ‘åªè¦å‘Šè¯‰è€å¸ˆï¼Œæ˜¯æœºå™¨äººå†™çš„ï¼â€\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# å®šä¹‰ LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# é€šè¿‡ Prompt è°ƒç”¨ LLM\n",
    "ret = llm.invoke(template.format(subject='å°æ˜'))\n",
    "# æ‰“å°è¾“å‡º\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ChatPromptTemplate ç”¨æ¨¡æ¿è¡¨ç¤ºçš„å¯¹è¯ä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯èšå®¢AIç ”ç©¶é™¢çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«å¤§å‰', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={})]\n",
      "æˆ‘æ˜¯å¤§å‰ï¼Œèšå®¢AIç ”ç©¶é™¢çš„å®¢æœåŠ©æ‰‹ã€‚å¾ˆé«˜å…´ä¸ºæ‚¨æä¾›å¸®åŠ©ï¼è¯·é—®æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºæ‚¨åšçš„å‘¢ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = template.format_messages(\n",
    "    product=\"èšå®¢AIç ”ç©¶é™¢\",\n",
    "    name=\"å¤§å‰\",\n",
    "    query=\"ä½ æ˜¯è°\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "ret = llm.invoke(prompt)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MessagesPlaceholder æŠŠå¤šè½®å¯¹è¯å˜æˆæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    # variable_name æ˜¯ message placeholder åœ¨æ¨¡æ¿ä¸­çš„å˜é‡å\n",
    "    # ç”¨äºåœ¨èµ‹å€¼æ—¶ä½¿ç”¨\n",
    "    [MessagesPlaceholder(\"history\"), human_message_template]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is Elon Musk?', additional_kwargs={}, response_metadata={}), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer', additional_kwargs={}, response_metadata={}), HumanMessage(content='Translate your answer to ä¸­æ–‡.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    # å¯¹ \"history\" å’Œ \"language\" èµ‹å€¼\n",
    "    history=[human_message, ai_message], language=\"ä¸­æ–‡\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸƒéš†Â·é©¬æ–¯å…‹ï¼ˆElon Muskï¼‰æ˜¯ä¸€ä½äº¿ä¸‡å¯Œç¿ä¼ä¸šå®¶ã€å‘æ˜å®¶å’Œå·¥ä¸šè®¾è®¡å¸ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 ä»æ–‡ä»¶åŠ è½½ Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'\n",
      "===Prompt===\n",
      "ä¸¾ä¸€ä¸ªå…³äºé»‘è‰²å¹½é»˜çš„ä¾‹å­\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file(\"example_prompt_template.txt\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='é»‘è‰²å¹½é»˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ç»“æ„åŒ–è¾“å‡º\n",
    "\n",
    "#### 1.3.1 ç›´æ¥è¾“å‡º Pydantic å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date(year=2024, month=12, day=23, era='AD')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "model_name = 'gpt-4o-mini'\n",
    "temperature = 0\n",
    "llm = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹\n",
    "structured_llm = llm.with_structured_output(Date)\n",
    "\n",
    "template = \"\"\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "query = \"2024å¹´åäºŒæœˆ23æ—¥å¤©æ°”æ™´...\"\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "\n",
    "structured_llm.invoke(input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 è¾“å‡ºæŒ‡å®šæ ¼å¼çš„ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 23, 'month': 12, 'year': 2024}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Date\",\n",
    "    \"description\": \"Formated date expression\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"year, YYYY\",\n",
    "        },\n",
    "        \"month\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"month, MM\",\n",
    "        },\n",
    "        \"day\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"day, DD\",\n",
    "        },\n",
    "        \"era\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"BC or AD\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 ä½¿ç”¨ OutputParser\n",
    "\n",
    "[`OutputParser`](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) å¯ä»¥æŒ‰æŒ‡å®šæ ¼å¼è§£ææ¨¡å‹çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "```json\n",
      "{\"year\": 2024, \"month\": 12, \"day\": 23, \"era\": \"AD\"}\n",
      "```\n",
      "\n",
      "è§£æå:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'year': 2024, 'month': 12, 'day': 23, 'era': 'AD'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Date)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\\nç”¨æˆ·è¾“å…¥:{query}\\n{format_instructions}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\"+output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥ç”¨ `PydanticOutputParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "```json\n",
      "{\n",
      "  \"year\": 2024,\n",
      "  \"month\": 12,\n",
      "  \"day\": 23,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "```\n",
      "\n",
      "è§£æå:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2024, month=12, day=23, era='AD')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\"+output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OutputFixingParser` åˆ©ç”¨å¤§æ¨¡å‹åšæ ¼å¼è‡ªåŠ¨çº é”™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PydanticOutputParser:\n",
      "Invalid json output: ```json\n",
      "{\n",
      "  \"year\": 202å››,\n",
      "  \"month\": 12,\n",
      "  \"day\": 23,\n",
      "  \"era\": \"AD\"\n",
      "}\n",
      "```\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n",
      "OutputFixingParser:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2024, month=12, day=23, era='AD')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI(model=\"gpt-4o\"))\n",
    "\n",
    "bad_output = output.content.replace(\"4\",\"å››\")\n",
    "print(\"PydanticOutputParser:\")\n",
    "try:\n",
    "    parser.invoke(bad_output)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"OutputFixingParser:\")\n",
    "new_parser.invoke(bad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"multiply\",\n",
      "        \"args\": {\n",
      "            \"a\": 3,\n",
      "            \"b\": 4\n",
      "        },\n",
      "        \"id\": \"call_V6lxABmx12g6lIgT8WMUtexM\",\n",
      "        \"type\": \"tool_call\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm_with_tools = llm.bind_tools([add, multiply])\n",
    "\n",
    "query = \"3çš„4å€æ˜¯å¤šå°‘?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "output = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(json.dumps(output.tool_calls, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›ä¼  Funtion Call çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"content\": \"3çš„4å€æ˜¯å¤šå°‘?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"human\",\n",
      "    \"name\": null,\n",
      "    \"id\": null,\n",
      "    \"example\": false\n",
      "}\n",
      "{\n",
      "    \"content\": \"\",\n",
      "    \"additional_kwargs\": {\n",
      "        \"tool_calls\": [\n",
      "            {\n",
      "                \"id\": \"call_V6lxABmx12g6lIgT8WMUtexM\",\n",
      "                \"function\": {\n",
      "                    \"arguments\": \"{\\\"a\\\":3,\\\"b\\\":4}\",\n",
      "                    \"name\": \"multiply\"\n",
      "                },\n",
      "                \"type\": \"function\"\n",
      "            }\n",
      "        ],\n",
      "        \"refusal\": null\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "        \"token_usage\": {\n",
      "            \"completion_tokens\": 18,\n",
      "            \"prompt_tokens\": 97,\n",
      "            \"total_tokens\": 115,\n",
      "            \"completion_tokens_details\": {\n",
      "                \"accepted_prediction_tokens\": 0,\n",
      "                \"audio_tokens\": 0,\n",
      "                \"reasoning_tokens\": 0,\n",
      "                \"rejected_prediction_tokens\": 0\n",
      "            },\n",
      "            \"prompt_tokens_details\": {\n",
      "                \"audio_tokens\": 0,\n",
      "                \"cached_tokens\": 0\n",
      "            }\n",
      "        },\n",
      "        \"model_name\": \"gpt-4o-mini-2024-07-18\",\n",
      "        \"system_fingerprint\": \"fp_0aa8d3e20b\",\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"name\": null,\n",
      "    \"id\": \"run-d25ca9ee-50b1-4848-a79e-42e58803fc7a-0\",\n",
      "    \"example\": false,\n",
      "    \"tool_calls\": [\n",
      "        {\n",
      "            \"name\": \"multiply\",\n",
      "            \"args\": {\n",
      "                \"a\": 3,\n",
      "                \"b\": 4\n",
      "            },\n",
      "            \"id\": \"call_V6lxABmx12g6lIgT8WMUtexM\",\n",
      "            \"type\": \"tool_call\"\n",
      "        }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "        \"input_tokens\": 97,\n",
      "        \"output_tokens\": 18,\n",
      "        \"total_tokens\": 115,\n",
      "        \"input_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"cache_read\": 0\n",
      "        },\n",
      "        \"output_token_details\": {\n",
      "            \"audio\": 0,\n",
      "            \"reasoning\": 0\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"content\": \"12\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"tool\",\n",
      "    \"name\": \"multiply\",\n",
      "    \"id\": null,\n",
      "    \"tool_call_id\": \"call_V6lxABmx12g6lIgT8WMUtexM\",\n",
      "    \"artifact\": null,\n",
      "    \"status\": \"success\"\n",
      "}\n",
      "3çš„4å€æ˜¯12ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16608\\2298449061.py:12: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  print(json.dumps(message.dict(), indent=4, ensure_ascii=False))\n"
     ]
    }
   ],
   "source": [
    "messages.append(output)\n",
    "\n",
    "available_tools = {\"add\": add, \"multiply\": multiply}\n",
    "\n",
    "for tool_call in output.tool_calls:\n",
    "    selected_tool = available_tools[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "new_output = llm_with_tools.invoke(messages)\n",
    "for message in messages:\n",
    "    print(json.dumps(message.dict(), indent=4, ensure_ascii=False))\n",
    "print(new_output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5ã€å°ç»“\n",
    "\n",
    "1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§\n",
    "2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿\n",
    "3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡\n",
    "4. LangChain æä¾›äº† Function Calling çš„å°è£…\n",
    "5. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºå®ç”¨çš„éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "<img src=\"./assets/data_connection.jpg\" style=\"margin-left: 0px\" width=800px>\n",
    "\n",
    "### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "#### 2.2.1 TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Hugo Touvronâˆ—\n",
      "Louis Martinâ€ \n",
      "Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "-------\n",
      "Kevin Stoneâ€ \n",
      "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "-------\n",
      "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "-------\n",
      "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "-------\n",
      "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "-------\n",
      "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "-------\n",
      "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "-------\n",
      "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "-------\n",
      "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "-------\n",
      "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "-------\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov\n",
      "Thomas Scialomâˆ—\n",
      "-------\n",
      "Sergey Edunov\n",
      "Thomas Scialomâˆ—\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "-------\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ï¬ne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "-------\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "-------\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "-------\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "-------\n",
      "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "-------\n",
      "source models. We provide a detailed description of our approach to ï¬ne-tuning and safety\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "-------\n",
      "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
      "contribute to the responsible development of LLMs.\n",
      "-------\n",
      "contribute to the responsible development of LLMs.\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "-------\n",
      "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "â€ Second author\n",
      "Contributions for all the authors can be found in Section A.1.\n",
      "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ç®€å•çš„æ–‡æœ¬å†…å®¹åˆ‡å‰²\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100, \n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "ç±»ä¼¼ LlamaIndexï¼ŒLangChain ä¹Ÿæä¾›äº†ä¸°å¯Œçš„ <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#document-loaders\">Document Loaders</a></code> å’Œ <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#text-splitters\">Text Splitters</a></code>ã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3ã€å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c pytorch faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but are not releasing.Â§\n",
      "2. Llama 2-Chat, a ï¬ne-tuned version of Llama 2 that is optimized for dialogue use cases. We release\n",
      "variants of this model with 7B, 13B, and 70B parameters as well.\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneï¬t to society. Like all LLMs,\n",
      "----\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "----\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ï¬ne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-3 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "docs = retriever.invoke(\"llama2æœ‰å¤šå°‘å‚æ•°\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ä¸‰æ–¹æ£€ç´¢ç»„ä»¶é“¾æ¥ï¼Œå‚è€ƒï¼šhttps://python.langchain.com/v0.3/docs/integrations/vectorstores/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4ã€å°ç»“\n",
    "\n",
    "1. æ–‡æ¡£å¤„ç†éƒ¨åˆ†ï¼Œå»ºè®®åœ¨å®é™…åº”ç”¨ä¸­è¯¦ç»†æµ‹è¯•åä½¿ç”¨\n",
    "2. ä¸å‘é‡æ•°æ®åº“çš„é“¾æ¥éƒ¨åˆ†æœ¬è´¨æ˜¯æ¥å£å°è£…ï¼Œå‘é‡æ•°æ®åº“éœ€è¦è‡ªå·±é€‰å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€å¯¹è¯å†å²ç®¡ç†\n",
    "\n",
    "### 3.1ã€å†å²è®°å½•çš„å‰ªè£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n",
    "    HumanMessage(\"i wonder why it's called langchain\"),\n",
    "    AIMessage(\n",
    "        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyways\"),\n",
    "    AIMessage(\n",
    "        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n",
    "    ),\n",
    "    HumanMessage(\"what do you call a speechless parrot\"),\n",
    "]\n",
    "\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant, you always respond with a joke.\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what do you call a speechless parrot', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¿ç•™ system prompt\n",
    "trim_messages(\n",
    "    messages,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    include_system=True,\n",
    "    allow_partial=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2ã€è¿‡æ»¤å¸¦æ ‡è¯†çš„å†å²è®°å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "\n",
    "filter_messages(messages, include_types=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='you are a good assistant', additional_kwargs={}, response_metadata={}, id='1'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='example input', additional_kwargs={}, response_metadata={}, name='example_user', id='2'),\n",
       " HumanMessage(content='real input', additional_kwargs={}, response_metadata={}, name='bob', id='4'),\n",
       " AIMessage(content='real output', additional_kwargs={}, response_metadata={}, name='alice', id='5')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_messages(messages, include_types=[HumanMessage, AIMessage], exclude_ids=[\"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››ã€Chain å’Œ LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚\n",
    "\n",
    "LCEL çš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä» LLM æµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚\n",
    "\n",
    "2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚\n",
    "\n",
    "3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚\n",
    "\n",
    "4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚\n",
    "\n",
    "5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚\n",
    "\n",
    "6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "7. **æ— ç¼ LangSmith è·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚\n",
    "\n",
    "8. **æ— ç¼ LangServe éƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚\n",
    "\n",
    "åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Pipeline å¼è°ƒç”¨ PromptTemplate, LLM å’Œ OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_16608\\4198727415.py:44: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  ret.dict(),\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºç»“æ„\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµé‡åŒ…åç§°\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(description=\"å‡åºæˆ–é™åºæ’åˆ—\", default=None)\n",
    "\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰è§£æå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚ä¸è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(Semantics)\n",
    "\n",
    "# LCEL è¡¨è¾¾å¼\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | structured_llm\n",
    ")\n",
    "\n",
    "# ç›´æ¥è¿è¡Œ\n",
    "ret = runnable.invoke(\"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›\")\n",
    "print(\n",
    "    json.dumps(\n",
    "        ret.dict(),\n",
    "        indent = 4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¥½çš„ï¼Œé‚£æˆ‘å°±ç»™ä½ è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯å§ï¼\n",
      "\n",
      "ä¸€å¤©ï¼Œè€å¸ˆé—®å°æ˜ï¼šâ€œå¦‚æœåœ°çƒæ˜¯æ–¹çš„ï¼Œé‚£ä¼šæ€ä¹ˆæ ·ï¼Ÿâ€\n",
      "\n",
      "å°æ˜æ€è€ƒäº†ä¸€ä¼šå„¿ï¼Œéå¸¸è®¤çœŸåœ°å›ç­”ï¼šâ€œé‚£æˆ‘å°æ—¶å€™ç©æ‰è¿·è—å°±ä¸ç”¨è¢«äººå‘ç°äº†ï¼Œå› ä¸ºæˆ‘å¯ä»¥ç›´æ¥èº²åœ¨åœ°çƒçš„æ‹è§’å¤„ï¼â€\n",
      "\n",
      "è€å¸ˆå·®ç‚¹ç¬‘åˆ°æ‰ä¹¦ï¼"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"è®²ä¸ªå…³äº{topic}çš„ç¬‘è¯\")\n",
    "\n",
    "runnable = (\n",
    "    {\"topic\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# æµå¼è¾“å‡º\n",
    "for s in runnable.stream(\"å°æ˜\"):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ³¨æ„:</b> åœ¨å½“å‰çš„æ–‡æ¡£ä¸­ LCEL äº§ç”Ÿçš„å¯¹è±¡ï¼Œè¢«å«åš runnable æˆ– chainï¼Œç»å¸¸ä¸¤ç§å«æ³•æ··ç”¨ã€‚æœ¬è´¨å°±æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰è°ƒç”¨æµç¨‹ã€‚\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />\n",
    "å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼š<a href=\"https://python.langchain.com/v0.1/docs/expression_language/why/\">https://python.langchain.com/v0.1/docs/expression_language/why/</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ç”¨ LCEL å®ç° RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"llama2.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:4]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-2 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ŒLlama 2 æœ‰ 7Bï¼ˆ70äº¿ï¼‰ã€13Bï¼ˆ130äº¿ï¼‰å’Œ 70Bï¼ˆ700äº¿ï¼‰å‚æ•°çš„ä¸åŒç‰ˆæœ¬ã€‚'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"Llama 2æœ‰å¤šå°‘å‚æ•°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ç”¨ LCEL å®ç°å·¥å‚æ¨¡å¼ï¼ˆé€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "# æ¨¡å‹1\n",
    "ernie_model = QianfanChatEndpoint(\n",
    "    qianfan_ak=os.getenv('ERNIE_CLIENT_ID'),\n",
    "    qianfan_sk=os.getenv('ERNIE_CLIENT_SECRET')\n",
    ")\n",
    "\n",
    "# æ¨¡å‹2\n",
    "gpt_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "# é€šè¿‡ configurable_alternatives æŒ‰æŒ‡å®šå­—æ®µé€‰æ‹©æ¨¡å‹\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"), \n",
    "    default_key=\"gpt\", \n",
    "    ernie=ernie_model,\n",
    "    # claude=claude_model,\n",
    ")\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LCEL\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# è¿è¡Œæ—¶æŒ‡å®šæ¨¡å‹ \"gpt\" or \"ernie\"\n",
    "ret = chain.with_config(configurable={\"llm\": \"gpt\"}).invoke(\"è¯·è‡ªæˆ‘ä»‹ç»\")\n",
    "\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰©å±•é˜…è¯»ï¼šä»€ä¹ˆæ˜¯[**å·¥å‚æ¨¡å¼**](https://www.runoob.com/design-pattern/factory-pattern.html)ï¼›[**è®¾è®¡æ¨¡å¼**](https://www.runoob.com/design-pattern/design-pattern-intro.html)æ¦‚è§ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 å­˜å‚¨ä¸ç®¡ç†å¯¹è¯å†å²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id):\n",
    "    # é€šè¿‡ session_id åŒºåˆ†å¯¹è¯å†å²ï¼Œå¹¶å­˜å‚¨åœ¨ sqlite æ•°æ®åº“ä¸­\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\envs\\langchain-learn\\lib\\site-packages\\langchain_core\\runnables\\history.py:608: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
      "  message_history = self.get_session_history(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ï¼Œå¤§æ‹¿ï¼å¾ˆé«˜å…´å†æ¬¡è§åˆ°ä½ ã€‚æœ‰ä»»ä½•é—®é¢˜æˆ–æƒ³èŠçš„è¯é¢˜å—ï¼Ÿ'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "runnable = model | StrOutputParser()\n",
    "\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    runnable, # æŒ‡å®š runnable\n",
    "    get_session_history, # æŒ‡å®šè‡ªå®šä¹‰çš„å†å²ç®¡ç†æ–¹æ³•\n",
    ")\n",
    "\n",
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘å«å¤§æ‹¿\")],\n",
    "    config={\"configurable\": {\"session_id\": \"dana\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æ˜¯çš„ï¼Œä½ å«å¤§æ‹¿ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥ä¸ºä½ åšçš„å‘¢ï¼Ÿ'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"ä½ çŸ¥é“æˆ‘å«ä»€ä¹ˆåå­—\")],\n",
    "    config={\"configurable\": {\"session_id\": \"dana\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æŠ±æ­‰ï¼Œæˆ‘ä¸çŸ¥æ‚¨çš„åå­—ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘æ‚¨çš„åå­—ã€‚'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable_with_history.invoke(\n",
    "    [HumanMessage(content=\"ä½ çŸ¥é“æˆ‘å«ä»€ä¹ˆåå­—\")],\n",
    "    config={\"configurable\": {\"session_id\": \"test\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°\n",
    "\n",
    "1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/v0.3/docs/how_to/configure/\n",
    "2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/v0.3/docs/how_to/fallbacks\n",
    "3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/v0.3/docs/how_to/parallel/\n",
    "4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/v0.3/docs/how_to/routing/\n",
    "5. åŠ¨æ€åˆ›å»º Chain: https://python.langchain.com/v0.3/docs/how_to/dynamic_chain/\n",
    "\n",
    "æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/v0.3/docs/how_to/lcel_cheatsheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äº”ã€LangServe\n",
    "\n",
    "LangServe ç”¨äºå°† Chain æˆ–è€… Runnable éƒ¨ç½²æˆä¸€ä¸ª REST API æœåŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£… LangServe\n",
    "# !pip install --upgrade \"langserve[all]\"\n",
    "\n",
    "# ä¹Ÿå¯ä»¥åªå®‰è£…ä¸€ç«¯\n",
    "# !pip install \"langserve[client]\"\n",
    "# !pip install \"langserve[server]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1ã€Server ç«¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_template(\"è®²ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯\")\n",
    "add_routes(\n",
    "    app,\n",
    "    prompt | model,\n",
    "    path=\"/joke\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=9999)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2ã€Client ç«¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:9999/joke/invoke\",\n",
    "    json={'input': {'topic': 'å°æ˜'}}\n",
    ")\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain ä¸ LlamaIndex çš„é”™ä½ç«äº‰\n",
    "\n",
    "- LangChain ä¾§é‡ä¸ LLM æœ¬èº«äº¤äº’çš„å°è£…\n",
    "  - Promptã€LLMã€Messageã€OutputParser ç­‰å·¥å…·ä¸°å¯Œ\n",
    "  - åœ¨æ•°æ®å¤„ç†å’Œ RAG æ–¹é¢æä¾›çš„å·¥å…·ç›¸å¯¹ç²—ç³™\n",
    "  - ä¸»æ‰“ LCEL æµç¨‹å°è£…\n",
    "  - é…å¥— Agentã€LangGraph ç­‰æ™ºèƒ½ä½“ä¸å·¥ä½œæµå·¥å…·\n",
    "  - å¦æœ‰ LangServe éƒ¨ç½²å·¥å…·å’Œ LangSmith ç›‘æ§è°ƒè¯•å·¥å…·\n",
    "- LlamaIndex ä¾§é‡ä¸æ•°æ®äº¤äº’çš„å°è£…\n",
    "  - æ•°æ®åŠ è½½ã€åˆ‡å‰²ã€ç´¢å¼•ã€æ£€ç´¢ã€æ’åºç­‰ç›¸å…³å·¥å…·ä¸°å¯Œ\n",
    "  - Promptã€LLM ç­‰åº•å±‚å°è£…ç›¸å¯¹å•è–„\n",
    "  - é…å¥—å®ç° RAG ç›¸å…³å·¥å…·\n",
    "  - æœ‰ Agent ç›¸å…³å·¥å…·ï¼Œä¸çªå‡º\n",
    "- LlamaIndex ä¸º LangChain æä¾›äº†é›†æˆ\n",
    "  - åœ¨ LlamaIndex ä¸­è°ƒç”¨ LangChain å°è£…çš„ LLM æ¥å£ï¼šhttps://docs.llamaindex.ai/en/stable/api_reference/llms/langchain/\n",
    "  - å°† LlamaIndex çš„ Query Engine ä½œä¸º LangChain Agent çš„å·¥å…·ï¼šhttps://docs.llamaindex.ai/en/v0.10.17/community/integrations/using_with_langchain.html\n",
    "  - LangChain ä¹Ÿ _æ›¾ç»_ é›†æˆè¿‡ LlamaIndexï¼Œç›®å‰ç›¸å…³æ¥å£ä»åœ¨ï¼šhttps://api.python.langchain.com/en/latest/retrievers/langchain_community.retrievers.llama_index.LlamaIndexRetriever.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "1. LangChain éšç€ç‰ˆæœ¬è¿­ä»£å¯ç”¨æ€§æœ‰æ˜æ˜¾æå‡\n",
    "2. ä½¿ç”¨ LangChain è¦æ³¨æ„ç»´æŠ¤è‡ªå·±çš„ Promptï¼Œå°½é‡ Prompt ä¸ä»£ç é€»è¾‘è§£ä¾èµ–\n",
    "3. å®ƒçš„å†…ç½®åŸºç¡€å·¥å…·ï¼Œå»ºè®®å……åˆ†æµ‹è¯•æ•ˆæœåå†å†³å®šæ˜¯å¦ä½¿ç”¨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
